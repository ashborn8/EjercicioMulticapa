{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPEWRYYIF7nO7QKc/qm8BUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashborn8/EjercicioMulticapa/blob/main/Ejercicio2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importar las bibliotecas necesarias\n",
        "import torch  # Funciones básicas para computación numérica\n",
        "import torch.nn as nn  # Clases y funciones para redes neuronales\n",
        "import torch.optim as optim  # Algoritmos de optimización\n",
        "from torch.utils.data import DataLoader  # Carga datos en lotes\n",
        "from torchvision import datasets, transforms  # Herramientas para datasets de imágenes"
      ],
      "metadata": {
        "id": "RHUDvJ1Nk9QR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Definir los hiperparámetros\n",
        "input_size = 784  # Tamaño de entrada (784 píxeles de la imagen)\n",
        "hidden_size = 128  # Número de neuronas en la capa oculta\n",
        "output_size = 10   # Número de clases (0-9)\n",
        "batch_size = 64    # Tamaño del lote para el entrenamiento\n",
        "learning_rate = 0.001  # Tasa de aprendizaje\n",
        "num_epochs = 5     # Número de épocas de entrenamiento"
      ],
      "metadata": {
        "id": "Lz3CMGN8lH1r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Descargar y cargar el dataset MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # Transformaciones para las imágenes: convertir a tensor y normalizar\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)  # Dataset de entrenamiento\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)  # Dataset de prueba\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)  # DataLoader para entrenamiento (mezcla los datos)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # DataLoader para prueba (no mezcla los datos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcgkHQRnlKz5",
        "outputId": "89c10ecb-d053-4653-e6e4-cb6510cfaa02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35055583.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1088283.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 8206503.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5892523.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Definir el modelo de red neuronal\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Capa completamente conectada 1\n",
        "        self.relu = nn.ReLU() # Función de activación ReLU\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Capa completamente conectada 2 (salida)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, input_size)  # Aplana la imagen a un vector\n",
        "        x = self.fc1(x)             # Pasa por la primera capa\n",
        "        x = self.relu(x)            # Aplica ReLU\n",
        "        x = self.fc2(x)             # Pasa por la capa de salida\n",
        "        return x\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size) # Crea una instancia del modelo"
      ],
      "metadata": {
        "id": "e_pny1K6lNth"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Definir la función de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()  # Función de pérdida para clasificación multiclase\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizador Adam"
      ],
      "metadata": {
        "id": "89IjHnm3lQXk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Entrenar el modelo\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)  # Pasa las imágenes por el modelo\n",
        "        loss = criterion(outputs, labels)  # Calcula la pérdida\n",
        "\n",
        "        # Backward pass y optimización\n",
        "        optimizer.zero_grad()  # Reinicia los gradientes\n",
        "        loss.backward()        # Calcula los gradientes\n",
        "        optimizer.step()       # Actualiza los pesos del modelo\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Época [{epoch+1}/{num_epochs}], Lote [{i+1}/{len(train_loader)}], Pérdida: {loss.item():.4f}') # Imprime la pérdida cada 100 lotes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzvmc-0BliXJ",
        "outputId": "914e0891-f5d6-47d4-9e6c-ba8021a28178"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [1/5], Lote [100/938], Pérdida: 0.3786\n",
            "Época [1/5], Lote [200/938], Pérdida: 0.4125\n",
            "Época [1/5], Lote [300/938], Pérdida: 0.3356\n",
            "Época [1/5], Lote [400/938], Pérdida: 0.3359\n",
            "Época [1/5], Lote [500/938], Pérdida: 0.1618\n",
            "Época [1/5], Lote [600/938], Pérdida: 0.5256\n",
            "Época [1/5], Lote [700/938], Pérdida: 0.2918\n",
            "Época [1/5], Lote [800/938], Pérdida: 0.2642\n",
            "Época [1/5], Lote [900/938], Pérdida: 0.2479\n",
            "Época [2/5], Lote [100/938], Pérdida: 0.1451\n",
            "Época [2/5], Lote [200/938], Pérdida: 0.0753\n",
            "Época [2/5], Lote [300/938], Pérdida: 0.2495\n",
            "Época [2/5], Lote [400/938], Pérdida: 0.2076\n",
            "Época [2/5], Lote [500/938], Pérdida: 0.0631\n",
            "Época [2/5], Lote [600/938], Pérdida: 0.2761\n",
            "Época [2/5], Lote [700/938], Pérdida: 0.2347\n",
            "Época [2/5], Lote [800/938], Pérdida: 0.1440\n",
            "Época [2/5], Lote [900/938], Pérdida: 0.1249\n",
            "Época [3/5], Lote [100/938], Pérdida: 0.2402\n",
            "Época [3/5], Lote [200/938], Pérdida: 0.1942\n",
            "Época [3/5], Lote [300/938], Pérdida: 0.2052\n",
            "Época [3/5], Lote [400/938], Pérdida: 0.1832\n",
            "Época [3/5], Lote [500/938], Pérdida: 0.1647\n",
            "Época [3/5], Lote [600/938], Pérdida: 0.0887\n",
            "Época [3/5], Lote [700/938], Pérdida: 0.1394\n",
            "Época [3/5], Lote [800/938], Pérdida: 0.0972\n",
            "Época [3/5], Lote [900/938], Pérdida: 0.1356\n",
            "Época [4/5], Lote [100/938], Pérdida: 0.2084\n",
            "Época [4/5], Lote [200/938], Pérdida: 0.0846\n",
            "Época [4/5], Lote [300/938], Pérdida: 0.0811\n",
            "Época [4/5], Lote [400/938], Pérdida: 0.1144\n",
            "Época [4/5], Lote [500/938], Pérdida: 0.1241\n",
            "Época [4/5], Lote [600/938], Pérdida: 0.2112\n",
            "Época [4/5], Lote [700/938], Pérdida: 0.1546\n",
            "Época [4/5], Lote [800/938], Pérdida: 0.0779\n",
            "Época [4/5], Lote [900/938], Pérdida: 0.1311\n",
            "Época [5/5], Lote [100/938], Pérdida: 0.0584\n",
            "Época [5/5], Lote [200/938], Pérdida: 0.0409\n",
            "Época [5/5], Lote [300/938], Pérdida: 0.1636\n",
            "Época [5/5], Lote [400/938], Pérdida: 0.0211\n",
            "Época [5/5], Lote [500/938], Pérdida: 0.0396\n",
            "Época [5/5], Lote [600/938], Pérdida: 0.1105\n",
            "Época [5/5], Lote [700/938], Pérdida: 0.1704\n",
            "Época [5/5], Lote [800/938], Pérdida: 0.1342\n",
            "Época [5/5], Lote [900/938], Pérdida: 0.0997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluar el modelo\n",
        "model.eval()  # Pone el modelo en modo de evaluación (desactiva capas como Dropout)\n",
        "with torch.no_grad():  # Desactiva el cálculo de gradientes (ahorra memoria y acelera la evaluación)\n",
        "    correct = 0  # Inicializa el contador de predicciones correctas\n",
        "    total = 0    # Inicializa el contador total de predicciones\n",
        "    for images, labels in test_loader:  # Itera sobre los datos de prueba\n",
        "        outputs = model(images)  # Obtiene las predicciones del modelo\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Obtiene la clase predicha (la de mayor probabilidad)\n",
        "        total += labels.size(0)  # Incrementa el contador total con el número de imágenes en el lote\n",
        "        correct += (predicted == labels).sum().item()  # Incrementa el contador de correctas si la predicción coincide con la etiqueta real\n",
        "\n",
        "    print(f'Precisión en el conjunto de prueba: {100 * correct / total}%')  # Imprime la precisión del modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SY7Kpq6llMF",
        "outputId": "62a473dd-2cb0-41ea-b3cc-225b342a6a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión en el conjunto de prueba: 96.64%\n"
          ]
        }
      ]
    }
  ]
}